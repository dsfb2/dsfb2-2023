{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assigment 3\n",
        "Submission DDL: 22.11.2023 23:59.\n",
        "\n",
        "* For the tasks that require the text answer use *Insert* > *Text cell* and provide your answer in this cell. Text cell supports Markdown.\n",
        "* Submit your work to the submission box on MyCourses. You should submit only the **.ipynb file** with your code.\n",
        "* Before submission use *Runtime* > *Run all* to make sure that your notebook runs without errors.\n",
        "* To download the file from Google Colab use *File* > *Download* > *Download .ipynb*.\n",
        "* Submit the file with the name: A3_NAME_SURNAME_STUDENT NUMBER.ipynb\n",
        "\n",
        "**NB! Before editing the file, save a local copy to your Google Drive, otherwise your progress will be lost**"
      ],
      "metadata": {
        "id": "wytJzAvRbX3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Before starting the assignment make sure, that GPU aceleration is activated to benefit from lower computational times. Go to *Runtime* > *Change runtime type* and select *T4 GPU***. If you are running the code locally, later in code we will try to activate GPU computing if it is available.\n",
        "\n",
        "The assignment contains 2 main tasks:\n",
        "* **A** – Implement the Naive Bayes Classifier **(10 points)**\n",
        "* **B** – Implement the Transformer **(15 points)**\n",
        "\n",
        "Further description and specific instructions are provided within the course of the assignment. Places where you need to write your code are commented with capital leters, e.g. #YOUR CODE HERE"
      ],
      "metadata": {
        "id": "CqXJ_gVobw9z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the assignment you will be classifying hotel reviews from the Tripadvisor dataset."
      ],
      "metadata": {
        "id": "Cy4r2Zcwb1Br"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# init deterministic seed\n",
        "\n",
        "seed = 42 # YOUR CODE HERE # INPUT YOUR STUDENT NUMBER HERE, omit the letters\n",
        "assert type(seed) is int, \"Exclude letters, leave only numbers\"\n"
      ],
      "metadata": {
        "id": "MDBIEYP4b0h9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import dependencies\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina' # high-resolution plots\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "\n",
        "# url to our dataset\n",
        "url = 'https://raw.githubusercontent.com/dsfb2/dsfb2-2023/main/assignment_3/data/tripadvisor_hotel_reviews.csv'\n",
        "\n",
        "# fix seed\n",
        "np.random.seed(seed)"
      ],
      "metadata": {
        "id": "oskMI9y1WXi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK A** – Implement the Naive Bayes Classifier **(10 points)**\n",
        "\n",
        "- **A1** - explore the dataset and consider what changes you should do. Write a short note how you should convert the ratings (Tip: computers start from 0) (**3 points**)\n",
        "- **A2** - build the Naive Bayes Classifier and report the results. Please, include, at least, the accuracy score, the classification report and the confusion matrix. Write a short report (roughly 10 sentences) about the result you get. (**7 points**)\n",
        "\n",
        "For this task there are no code snippets provide, you can take code parts from the tutorial"
      ],
      "metadata": {
        "id": "2mPAlrhEcqmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add as many cells as you need"
      ],
      "metadata": {
        "id": "O6QMGpqgeEJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK B** – Implement the Transformer **(15 points)**\n",
        "\n",
        "- **B1** - create the TravelDataset class (**5 points**)\n",
        "- **B2** - train the Transformer (**5 points**)\n",
        "- **B3** - evaluate the performance on the epoch with the lowest error and report the results. Please, include, at least, the accuracy score, the classification report and the confusion matrix. Write a short report on the results and compare them with Naive Nayes Classifier (roughly 15 sentences). (**5 points**)\n",
        "\n",
        "The majority of the code is provided, look for # YOUR CODE HERE to complete functions. For obtaining results you should write your own code."
      ],
      "metadata": {
        "id": "TiKvCIlgfPft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install the library that contains checkpoints of models and tokenizers\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "OChctr5CJePR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizerFast\n",
        "\n",
        "# Defining some key variables that will be used later on in the training, you can select your own parameters\n",
        "\n",
        "MAX_LEN = # YOUR CODE HERE                # max length of sequence. we will use all 512 as our text articles are long.\n",
        "TRAIN_BATCH_SIZE = # YOUR CODE HERE       # how many sequences are included in the training batch\n",
        "VALID_BATCH_SIZE = # YOUR CODE HERE       # how many sequences are included in the validation batch\n",
        "EPOCHS = # YOUR CODE HERE                 # how many epochs we will use during the training process\n",
        "LEARNING_RATE =  # YOUR CODE HERE         # our learning rate\n",
        "TOKENIZER = BertTokenizerFast.from_pretrained('bert-base-uncased', lower=True) # our tokenizer"
      ],
      "metadata": {
        "id": "0V0HigTyMN-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "class TravelDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df):\n",
        "\n",
        "        self.labels =  # YOUR CODE HERE # create labels for each article\n",
        "        self.texts = # YOUR CODE HERE # create tokens for each article\n",
        "\n",
        "    def classes(self):\n",
        "        return self.labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def get_batch_labels(self, idx):\n",
        "        # Fetch a batch of labels\n",
        "        return np.array(self.labels[idx])\n",
        "\n",
        "    def get_batch_texts(self, idx):\n",
        "        # Fetch a batch of inputs\n",
        "        return self.texts[idx]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        batch_texts = self.get_batch_texts(idx)\n",
        "        batch_y = self.get_batch_labels(idx)\n",
        "\n",
        "        return batch_texts, batch_y"
      ],
      "metadata": {
        "id": "cRC5I21sNxHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_val = # YOUR CODE HERE\n",
        "\n",
        "print(f'Training set length is {len(df_train)} and validation set length {len(df_val)}')"
      ],
      "metadata": {
        "id": "bizsbF4BOtYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from transformers import BertModel\n",
        "\n",
        "class BertClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, dropout=0.3):\n",
        "\n",
        "        super(BertClassifier, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased') # pre-trained transformer\n",
        "        self.dropout = nn.Dropout(dropout)                         # pool with dropout\n",
        "        self.linear =  # YOUR CODE HERE                            # classification fully-connected layer, you can add additional layers if you want\n",
        "        self.relu = nn.ReLU()                                      # ReLU activation function\n",
        "\n",
        "    def forward(self, input_id, mask):\n",
        "\n",
        "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "        linear_output = self.linear(dropout_output)\n",
        "        final_layer = self.relu(linear_output)\n",
        "\n",
        "        return final_layer"
      ],
      "metadata": {
        "id": "5ZdxZke7Z4-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# function for training and validation\n",
        "def train_validate(model, train_data, val_data, learning_rate, epochs):\n",
        "\n",
        "    # create tokenized datasets for training and validation\n",
        "    train, val = # YOUR CODE HERE\n",
        "\n",
        "    # create loaders for tensors\n",
        "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
        "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=VALID_BATCH_SIZE)\n",
        "\n",
        "    # activate GPU computing\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu').type\n",
        "    print('[LOG] notebook with {} computation enabled'.format(str(device)))\n",
        "\n",
        "    # initialize loss function\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # initialize optimizer\n",
        "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # send model and loss function to computational device\n",
        "    model = # YOUR CODE HERE\n",
        "    criterion = # YOUR CODE HERE\n",
        "\n",
        "    # initialize empty lists for storing data\n",
        "    history_val_loss=[]     # average training loss for epoch\n",
        "    history_train_loss = [] # average validation loss for epoch\n",
        "    history_val_acc=[]      # training accuracy for epoch\n",
        "    history_train_acc = []  # validation accuracy for epoch\n",
        "\n",
        "    # training and validation cycle\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # set the model to the training mode (gradients are updated)\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        # initialize list for storing loss for each propagation\n",
        "        loss_train = []\n",
        "\n",
        "        # initilize lists for storing actual and predicted labels\n",
        "        train_label_list = []\n",
        "        train_output_list = []\n",
        "\n",
        "################## TRAINING ##################\n",
        "\n",
        "        # get our train input and label tensors for loader, tdqm is just a nice progress bar\n",
        "        for train_input, train_label in tqdm(train_dataloader):\n",
        "\n",
        "            # send training label, attention mask and id to device\n",
        "            train_label = # YOUR CODE HERE\n",
        "            mask =  # YOUR CODE HERE\n",
        "            input_id = # YOUR CODE HERE\n",
        "\n",
        "            # receive predicted label\n",
        "            output = # YOUR CODE HERE\n",
        "\n",
        "            # calculate the loss value between actual and predicted label\n",
        "            batch_loss = # YOUR CODE HERE\n",
        "\n",
        "            # store the loss\n",
        "            loss_train.append(batch_loss.item())\n",
        "\n",
        "            # save actual and predicted values\n",
        "            train_label_list.extend(train_label.cpu().detach().numpy().tolist())\n",
        "            train_output_list.extend(torch.sigmoid(output).cpu().detach().numpy().tolist())\n",
        "\n",
        "            # reset graph gradients\n",
        "            # YOUR CODE HERE\n",
        "\n",
        "            # run backward pass to update the weights\n",
        "            # YOUR CODE HERE\n",
        "\n",
        "            # update network paramaters\n",
        "            # YOUR CODE HERE\n",
        "\n",
        "        # calculate average training loss\n",
        "        total_loss_train = np.mean(loss_train)\n",
        "        # append average training loss\n",
        "        history_train_loss.append(total_loss_train)\n",
        "        # calculate training accuracy\n",
        "        acc_train = accuracy_score(np.array(train_label_list).astype(int), np.argmax(np.array(train_output_list), axis=1))\n",
        "        # append training accuracy\n",
        "        history_train_acc.append(acc_train)\n",
        "\n",
        "################## VALIDATION ##################\n",
        "\n",
        "        # initialize list for storing loss for each propagation\n",
        "        loss_val = []\n",
        "\n",
        "        # initilize lists for storing actual and predicted labels\n",
        "        val_label_list = []\n",
        "        val_output_list = []\n",
        "\n",
        "        # set the model to the validation mode (gradients are not updated)\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            for val_input, val_label in val_dataloader:\n",
        "\n",
        "                # send validation label, attention mask and id to device\n",
        "                val_label = # YOUR CODE HERE\n",
        "                mask =  # YOUR CODE HERE\n",
        "                input_id =  # YOUR CODE HERE\n",
        "\n",
        "                # receive predicted label\n",
        "                output =  # YOUR CODE HERE\n",
        "\n",
        "                # calculate the loss value between actual and predicted label\n",
        "                batch_loss =  # YOUR CODE HERE\n",
        "\n",
        "                # store the loss\n",
        "                loss_val.append(batch_loss.item())\n",
        "\n",
        "                # save actual and predicted values\n",
        "                val_label_list.extend(val_label.cpu().detach().numpy().tolist())\n",
        "                val_output_list.extend(torch.sigmoid(output).cpu().detach().numpy().tolist())\n",
        "\n",
        "        # calculate average validation loss\n",
        "        total_loss_val = np.mean(loss_val)\n",
        "        # append average validation loss\n",
        "        history_val_loss.append(total_loss_val)\n",
        "        # calculate validation accuracy\n",
        "        acc_val = accuracy_score(np.array(val_label_list).astype(int), np.argmax(np.array(val_output_list), axis=1))\n",
        "        # append validation accuracy\n",
        "        history_val_acc.append(acc_val)\n",
        "\n",
        "        print(f'Epochs: {epoch} | Train Loss: {total_loss_train: .3f} | Train Accuracy: {acc_train: .3f} | Val Loss: {total_loss_val: .3f} | Val Accuracy: {acc_val: .3f}')\n",
        "        model_name = f'{epoch}_news_classifier.pth'\n",
        "        torch.save(model.state_dict(), model_name)\n",
        "\n",
        "    return history_train_loss, history_val_loss, history_train_acc, history_val_acc\n"
      ],
      "metadata": {
        "id": "pQjEDJNEQPF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize our model\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "KjcKp29GT7HE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before training the model, make sure the GPU is activated. Expected runtime for one epoch is 5 minutes"
      ],
      "metadata": {
        "id": "bew0_F0bUQIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "yk4irJ7qUBZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ploting the loss\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "E6FhlzGyUNKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting the accuracy\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "YZDiboAXYMlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_data):\n",
        "\n",
        "    # create tokenized dataset\n",
        "    test = # YOUR CODE HERE\n",
        "\n",
        "    # create loaders for tensors\n",
        "    val_dataloader = torch.utils.data.DataLoader(test, batch_size=VALID_BATCH_SIZE)\n",
        "\n",
        "    # activate GPU computing\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu').type\n",
        "    print('[LOG] notebook with {} computation enabled'.format(str(device)))\n",
        "\n",
        "    # initialize loss function\n",
        "    criterion =  # YOUR CODE HERE\n",
        "\n",
        "    model =  # YOUR CODE HERE\n",
        "    criterion =  # YOUR CODE HERE\n",
        "\n",
        "    loss_val = []\n",
        "\n",
        "    val_label_list = []\n",
        "    val_output_list = []\n",
        "\n",
        "    # set the model to the validation mode (gradients are not updated)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for val_input, val_label in val_dataloader:\n",
        "\n",
        "            # send validation label, attention mask and id to device\n",
        "            val_label =  # YOUR CODE HERE\n",
        "            mask =  # YOUR CODE HERE\n",
        "            input_id =  # YOUR CODE HERE\n",
        "\n",
        "            # receive predicted label\n",
        "            output =  # YOUR CODE HERE\n",
        "\n",
        "            # calculate the loss value between actual and predicted label\n",
        "            batch_loss =  # YOUR CODE HERE\n",
        "            loss_val.append(batch_loss.item())\n",
        "\n",
        "            # store the loss\n",
        "            val_label_list.extend(val_label.cpu().detach().numpy().tolist())\n",
        "            val_output_list.extend(torch.sigmoid(output).cpu().detach().numpy().tolist())\n",
        "\n",
        "        # save actual and predicted values\n",
        "        total_loss_val = np.mean(loss_val)\n",
        "        acc_val = accuracy_score(np.array(val_label_list).astype(int), np.argmax(np.array(val_output_list), axis=1))\n",
        "\n",
        "    print(f'Test Accuracy: {acc_val: .3f}')\n",
        "\n",
        "    # return actual and predicted values\n",
        "    return np.array(val_label_list).astype(int), np.argmax(np.array(val_output_list), axis=1)"
      ],
      "metadata": {
        "id": "9OmoQU2bY-Ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load saved state\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "_BAhqSfZY6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate model\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "ri2Xn4ymaO8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create classification report\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "6YRX8ukzac9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create confusion matrix\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "pAMhxQRGakcy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}