{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assigment 4 - IML and xAI with shap\n",
        "Submission DDL: 01.12.2023 23:59.\n",
        "\n",
        "* Submit your work to the submission box on MyCourses. You should submit only the **.ipynb file** with your code.\n",
        "* Before submission use *Runtime* > *Run all* to make sure that your notebook runs without errors.\n",
        "* To download the file from Google Colab use *File* > *Download* > *Download .ipynb*.\n",
        "* Submit the file with the name: A4_NAME_SURNAME_STUDENT NUMBER.ipynb\n",
        "\n",
        "**NB! Before editing the file, save a local copy to your Google Drive, otherwise your progress will be lost**"
      ],
      "metadata": {
        "id": "TksXjGsZHXGC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment you will combine the knowledge acquired earlier in the course with the content of Module 4. The task contains 2 parts: explanation of a model with tabular data and explanation of a model with image/text data. You are encouraged to use the code from the previous tutorials as well as from your previous assignments. There is no specific task-related code provided. Main modules are imported for your convenience. You can load any other modules you find useful.\n",
        "Please, keep your code rather clean and understandable. Descriptions of tasks are provided after imports."
      ],
      "metadata": {
        "id": "LiEkGdgJIsAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install shap transformers datasets\n",
        "!pip install shap transformers datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-B-2OTxJhTM",
        "outputId": "5e589592-94cc-4286-b4d1-3ccc90d515f1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting shap\n",
            "  Downloading shap-0.43.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (532 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m532.9/532.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.11.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (1.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.66.1)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (23.2)\n",
            "Collecting slicer==0.0.7 (from shap)\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.58.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting pyarrow-hotfix (from datasets)\n",
            "  Downloading pyarrow_hotfix-0.5-py3-none-any.whl (7.8 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.41.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2023.3.post1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->shap) (1.16.0)\n",
            "Installing collected packages: slicer, pyarrow-hotfix, dill, multiprocess, shap, datasets\n",
            "Successfully installed datasets-2.15.0 dill-0.3.7 multiprocess-0.70.15 pyarrow-hotfix-0.5 shap-0.43.0 slicer-0.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_4FMd8qHTvv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.metrics import mean_squared_error,r2_score\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import shap\n",
        "\n",
        "student_number = # INPUT YOUR STUDENT NUMBER HERE, omit the letters\n",
        "assert type(student_number) is int, \"Exclude letters, leave only numbers\"\n",
        "\n",
        "np.random.seed(student_number)\n",
        "torch.manual_seed(student_number)\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task A - Tabular data - 15 points\n",
        "Your task is to build a model on some tabular data and explain predictions and  observations using shap. The task consists of the following parts:\n",
        "- **A1** - find the suitable dataset (contains at least 3 features), import it and describe. Provide vizualizations. Shortly describe the problem you're solving (e.g. predicting price, customer churn, etc) **(2 points)** \\\n",
        " *Recommendation:* find a simple and clean dataset with at least 3 features, preferrably with the continuous target variable - then you can apply the same explanations as in the tutorial. If you do some data processing, shortly describe it.\n",
        "- **A2** - build a non-interpretable predictive model (regression/classification, depending on your data, you need to select only one). Assess the performance of the model using the suitable metrics of your choice. Write a short report (several sentences) on the model performance **(5 points)** \\\n",
        " *Recommendation:* use XGBoost or RandomForestClassifier depending on your data. In terms of performance, try to achieve $R^2 \\geq 0.5$ for regression and better than random guessing accuracy for classification. For performance assessment use at least $R^2$ + MSE or accuracy/recall + confusion matrix. It is better to build a solid model on simple data rather rather than try to build some fancy model on fancy data.\n",
        "- **A3** - interprete the model results using shap. Write full interpretation for each feature for one of the observations. Show the waterfall plot for this observation. Show the bar plot for the mean absolute shap values as well. Shortly explain the meaning of that plot. **(8 points)** \\\n",
        " Recommendation: Correct formulation of interpretation is extreamely important, please check the lecture and the tutorial.\n",
        "\n",
        "*General recommendation:* You can follow the tutorial with the dataset of your choice. Keep your explanations rather simple. The main aim is for you to show that you understand how to build and to explain the model. Your code and short reports should show your understanding."
      ],
      "metadata": {
        "id": "Slfm3aIaK3m2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# starting code cell for part A"
      ],
      "metadata": {
        "id": "ooNgglplK4FU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task B - Image/Text data - 10 points\n",
        "\n",
        "Your task is to build a **classification** model on some image/text data and explain predictions and observations using shap. It is up to you whether you want to use image or text data, but you need to select only one.\n",
        "\n",
        "- **B1** - select the data and the model that you will use. Shortly describe what is the dataset, model type and model structure. Prepare the model and the data for the explanation (activate data loaders, load the checkpoint, etc.). **(5 points)**\\\n",
        " *Recommendation:* You don't need to show model training, you can upload the checkpoint from one of the models from previous assignments or from the web\n",
        "- **B2** - explain model predictions using shap. **(5 points)** \\\n",
        " *Recommendation:* For image data show the image_plot and describe which pixels are important for prediction of different classes. For text_data show the text plot and describe which words are important for prediction of different classes.\n",
        "\n",
        "*General recommendation:* The easiest and recommended way is to take the model from Assignment 2 (the VGG network), retrain it with `inplace=false` for ReLU (as shown in the tutorial) and use it for shap explanations. More advanced way is to use the transformer from Assignment 3. The hardest one is to find some custom model and dataset from the web. Grading does not depend on how advance option you take as the aim for you is to show your understanding rather than to build some complex models."
      ],
      "metadata": {
        "id": "OChCK8ASK4i_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# starting code cell for part B"
      ],
      "metadata": {
        "id": "KJDc_M9DK47c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}